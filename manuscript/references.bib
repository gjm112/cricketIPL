
@Manual{R2022language,
    title = "R: A Language and Environment for Statistical Computing",
    author = "R Core Team",
    organization = "R Foundation for Statistical Computing",
    address = "Vienna, Austria",
    year = 2022,
    url = "https://www.R-project.org/",
}

@article{Nesterov2009,
  title = "Primal-dual subgradient methods for convex problems",
  author = "Nesterov, Yurii",
  year = 2009,
  journal = "Mathematical Programming",
  volume = 120,
  pages = "221-259",
  url = "https://doi.org/10.1007/s10107-007-0149-x"
}

@article{PGAS,
  author  = {Fredrik Lindsten and Michael I. Jordan and Thomas B. Sch{{\"o}}n},
  title   = {Particle Gibbs with Ancestor Sampling},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {63},
  pages   = {2145--2184},
  url     = {http://jmlr.org/papers/v15/lindsten14a.html}
}

@article{LKJ,
title = {Generating random correlation matrices based on vines and extended onion method},
journal = {Journal of Multivariate Analysis},
volume = {100},
number = {9},
pages = {1989-2001},
year = {2009},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2009.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X09000876},
author = {Daniel Lewandowski and Dorota Kurowicka and Harry Joe},
keywords = {Dependence vines, Correlation matrix, Partial correlation, Onion method},
abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276–294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177–2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.}
}

@article{NUTS,
author = {Homan, Matthew D. and Gelman, Andrew},
title = {The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size ε and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more effciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter ε on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1593–1623},
numpages = {31},
keywords = {Bayesian inference, Hamiltonian Monte Carlo, Markov chain Monte Carlo, adaptive Monte Carlo, dual averaging}
}

